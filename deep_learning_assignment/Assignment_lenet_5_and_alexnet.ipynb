{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME8vHUsrE+SvRHbMtihW3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hellokrrish/deep_learning/blob/main/Assignment_lenet_5_and_alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRF7mGGgy9NE"
      },
      "outputs": [],
      "source": [
        "'''1. Explain the architecture of LeNet-5 and its significance in the field of deep learning\n",
        "   2. Describe the key components of LeNet-5 and their roles in the network\n",
        "   3.Discuss the limitations of LeNet-5 and how subsequent architectures like AlexNet addressed these limitations\n",
        "   4.Explain the architecture of AlexNet and its contributions to the advancement of deep learning\n",
        "   5.Compare and contrast the architectures of LeNet-5 and AlexNet. Discuss their similarities, differences, and respective contributions to the field of deep learning'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' LeNet-5 Architecture and Significance\n",
        "LeNet-5, a pioneering convolutional neural network (CNN) architecture, was developed in the 1990s by Yann LeCun and his colleagues. It was designed to recognize handwritten and machine-printed characters, demonstrating the effectiveness of deep learning for practical applications. LeNet-5 introduced several key features that have become standard in modern deep learning, including convolution, pooling, and hierarchical feature extraction. These concepts underpin the success of many modern deep learning models for image recognition and other tasks.\n",
        "\n",
        "Key Components of LeNet-5 and their Roles\n",
        "Convolutional Layers: These layers extract features from the input image by applying filters that slide across the image. LeNet-5 uses multiple convolutional layers to learn increasingly complex features, such as edges, lines, and shapes.\n",
        "Pooling Layers: These layers reduce the dimensionality of the data by summarizing the information in a local region of the feature maps. LeNet-5 typically uses average pooling, which computes the average value of each region. Pooling helps to reduce the computational cost and make the network more robust to small variations in the input image.\n",
        "Fully Connected Layers: These layers connect all neurons in one layer to all neurons in the next layer, allowing the network to learn complex relationships between the extracted features. LeNet-5 uses fully connected layers to classify the input image into different categories (e.g., digits 0-9).\n",
        "Output Layer: The final layer of LeNet-5 uses a softmax activation function to output probabilities for each possible class. The class with the highest probability is chosen as the predicted output.\n",
        "Limitations of LeNet-5 and How AlexNet Addressed Them\n",
        "Limited Depth: LeNet-5 has a relatively shallow architecture with only a few convolutional and fully connected layers. This limits its ability to learn complex features necessary for more challenging image recognition tasks.\n",
        "Small Image Sizes: LeNet-5 was designed for small, low-resolution images (typically 32x32 pixels). It may not perform well on modern datasets with larger and higher-resolution images.\n",
        "Computational Constraints: The computational power available in the 1990s limited the size and complexity of LeNet-5. Training deep learning models on large datasets requires significant computing resources.\n",
        "Architectures like AlexNet addressed these limitations in the following ways:\n",
        "\n",
        "Increased Depth: AlexNet has a much deeper architecture compared to LeNet-5, with more convolutional and fully connected layers. This allows it to learn more complex features and achieve higher accuracy on image recognition tasks.\n",
        "ReLU Activation: AlexNet uses the ReLU (Rectified Linear Unit) activation function, which has been shown to improve training speed and performance compared to the sigmoid or tanh functions typically used in LeNet-5.\n",
        "Data Augmentation: AlexNet employs data augmentation techniques such as random cropping and flipping images to artificially increase the size and diversity of the training data. This helps the network to generalize better and reduce overfitting.\n",
        "GPU Acceleration: AlexNet was one of the first deep learning models to leverage the power of GPUs for training. GPUs can significantly accelerate the training process, making it possible to train deeper and more complex models.\n",
        "AlexNet Architecture and Contributions\n",
        "AlexNet, introduced in 2012, is a landmark deep learning architecture that significantly advanced the field of image recognition. It achieved state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), outperforming previous methods by a significant margin. This success sparked a renewed interest in deep learning research and led to rapid advancements in various fields.\n",
        "\n",
        "Here are some of AlexNet's key contributions:\n",
        "\n",
        "Demonstrated the Effectiveness of Deep CNNs: AlexNet showed that deep CNNs with many layers can achieve superior performance on complex image recognition tasks.\n",
        "Paved the Way for Future Architectures: AlexNet's design principles and techniques, such as ReLU activation and data augmentation, have influenced the development of many subsequent CNN architectures like VGGNet, Inception, and ResNet.\n",
        "Boosted Deep Learning Research: The success of AlexNet led to a surge in deep learning research, attracting researchers from various disciplines and accelerating innovation in the field.\n",
        "Comparison of LeNet-5 and AlexNet\n",
        "Feature\tLeNet-5\tAlexNet\n",
        "Depth\tShallow (few layers)\tDeep (multiple convolutional and fully connected layers)\n",
        "Activation Function\tSigmoid/Tanh\tReLU\n",
        "Image Size\tSmall (e.g., 32x32 pixels)\tLarger (e.g., 224x224 pixels)\n",
        "Training Data\tSmaller datasets\tLarge-'''"
      ],
      "metadata": {
        "id": "IJB532L9zufL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}