{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND4/z9dKhtOtX6pReO6n/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hellokrrish/deep_learning/blob/main/Assignment_Forward_and_Backward_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2f76ILZuOrm"
      },
      "outputs": [],
      "source": [
        "# 1. Explain the concept of forward propagation in a neural network'\n",
        "\n",
        "'''forward propagation is the process of feeding input data through the layers of a neural network to generate an output. It mimics how information flows in a biological neuron, where signals are transmitted from one neuron to another. In a neural network, this involves:\n",
        "\n",
        "Input Layer: Receiving the initial data.\n",
        "Hidden Layers: Performing computations on the input data using weights and biases.\n",
        "Output Layer: Producing the final output of the network.'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.What is the purpose of the activation function in forward propagation\n",
        "\n",
        "'''Activation functions introduce non-linearity into the neural network. Without them, the network would simply be performing linear transformations, limiting its ability to learn complex patterns in data. Activation functions allow the network to:\n",
        "\n",
        "Learn non-linear relationships: Represent complex functions and patterns that cannot be captured by linear models.\n",
        "Introduce decision boundaries: Create regions in the input space where the network produces different outputs.\n",
        "Improve model expressiveness: Increase the network's ability to learn and represent a wider range of functions.'''\n"
      ],
      "metadata": {
        "id": "LN7CTlgIvJFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.' Describe the steps involved in the backward propagation (backpropagation) algorithm'\n",
        "\n",
        "'''Backpropagation is an algorithm used to train neural networks. It works by:\n",
        "\n",
        "Forward Pass: Feeding input data through the network to generate an output.\n",
        "Calculating the Error: Comparing the network's output to the expected output and calculating the error (loss).\n",
        "Backward Pass: Propagating the error backward through the network, layer by layer.\n",
        "Calculating Gradients: Computing the gradients of the loss function with respect to the weights and biases of each neuron.\n",
        "Updating Weights and Biases: Adjusting the weights and biases of each neuron using gradient descent or other optimization algorithms to minimize the error.'''\n"
      ],
      "metadata": {
        "id": "OQvjQm-mvV5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.' What is the purpose of the chain rule in backpropagation\n",
        "\n",
        "'''The chain rule is crucial in backpropagation because it allows us to calculate the gradients of the loss function with respect to\n",
        "the weights and biases of each neuron in the network. Since the loss function depends on the weights and biases indirectly\n",
        "through a chain of computations, the chain rule is used to break down the complex gradient calculation into a series of simpler derivatives.'''\n"
      ],
      "metadata": {
        "id": "56SZoMLLvhgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define input data\n",
        "inputs = np.array([1, 2, 3])\n",
        "\n",
        "# Define weights and biases for the hidden and output layers\n",
        "weights_hidden = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
        "biases_hidden = np.array([0.1, 0.2])\n",
        "weights_output = np.array([[0.7, 0.8]])\n",
        "biases_output = np.array([0.1])\n",
        "\n",
        "# Calculate hidden layer activations\n",
        "# Transpose weights_hidden to align dimensions for matrix multiplication\n",
        "hidden_layer_input = np.dot(inputs, weights_hidden.T) + biases_hidden\n",
        "hidden_layer_activation = 1 / (1 + np.exp(-hidden_layer_input))  # Sigmoid activation\n",
        "\n",
        "# Calculate output layer activation\n",
        "output_layer_input = np.dot(hidden_layer_activation, weights_output.T) + biases_output\n",
        "output = 1 / (1 + np.exp(-output_layer_input))  # Sigmoid activation\n",
        "\n",
        "print(\"Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHEfh0BSv4B_",
        "outputId": "14bc251f-753e-4418-8513-ff9e899b32f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: [0.80945392]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkK8ezv1wIla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}