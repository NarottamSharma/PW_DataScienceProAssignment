{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2bBsveW15RIu"
      },
      "outputs": [],
      "source": [
        "# 1) Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear activation functions. Why are nonlinear activation functions preferred in hidden layers\n",
        "\n",
        "'''Activation functions are mathematical functions applied to the weighted sum of inputs in a neuron. They introduce non-linearity to the network, enabling it to learn complex patterns and relationships in the data. Without activation functions, the neural network would be equivalent to a single-layer perceptron, limited to modeling only linear relationships.\n",
        "\n",
        "Comparison of Linear and Nonlinear Activation Functions\n",
        "\n",
        "Linear Activation Function:\n",
        "\n",
        "Outputs a linear function of the input.\n",
        "Doesn't introduce non-linearity, limiting the network's ability to learn complex patterns.\n",
        "Example: f(x) = x\n",
        "Nonlinear Activation Functions:\n",
        "\n",
        "Introduce non-linearity, allowing the network to learn complex patterns.\n",
        "Examples:\n",
        "Sigmoid: Maps inputs to a range between 0 and 1.\n",
        "ReLU: Outputs the input directly if positive, otherwise outputs 0.\n",
        "Tanh: Maps inputs to a range between -1 and 1.\n",
        "Why Nonlinear Activation Functions Are Preferred in Hidden Layers\n",
        "\n",
        "Nonlinear activation functions are preferred in hidden layers because they:\n",
        "\n",
        "Enable the network to learn complex patterns: By introducing non-linearity, the network can approximate any continuous function, making it more powerful and expressive.\n",
        "Improve gradient flow: Nonlinear activation functions often have gradients that do not vanish or saturate, allowing the network to learn more effectively during backpropagation.\n",
        "Introduce decision boundaries: Nonlinear activation functions can create decision boundaries that are not linear, enabling the network to classify data that is not linearly separable.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages and potential challenges.What is the purpose of the Tanh activation function? How does it differ from the Sigmoid activation function\n",
        "''' The sigmoid activation function, often represented as σ(x), is a mathematical function that maps any input value to a value between 0 and 1. It has an \"S\"-shaped curve that asymptotes to 0 for large negative numbers and 1 for large positive numbers.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Range: Outputs are between 0 and 1, making it suitable for binary classification problems where the output can be interpreted as a probability.\n",
        "Smooth and differentiable: This property is crucial for gradient-based optimization algorithms like backpropagation.\n",
        "Vanishing gradients: As the input values move towards the extremes, the gradient of the sigmoid function becomes very small. This can slow down the training process, especially in deep networks.\n",
        "Common Usage:\n",
        "\n",
        "Output layer of binary classification models: The sigmoid function's output range of 0 to 1 aligns well with the interpretation of probabilities in binary classification.\n",
        "Rectified Linear Unit (ReLU) Activation Function\n",
        "\n",
        "The ReLU activation function is defined as:\n",
        "\n",
        "f(x) = max(0, x)\n",
        "'''"
      ],
      "metadata": {
        "id": "2R-Mrlpl6kk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Discuss the significance of activation functions in the hidden layers of a neural network-\n",
        "\n",
        "''' Activation functions play a pivotal role in the hidden layers of a neural network, enabling them to learn complex patterns and represent intricate relationships within the data. Here's a breakdown of their significance:\n",
        "\n",
        "1. Introducing Non-Linearity:\n",
        "\n",
        "Linear Limitations: Without activation functions, a neural network would essentially be a linear model, capable of only learning linear relationships. This severely limits its ability to model complex, real-world data.\n",
        "Non-Linearity is Key: Activation functions introduce non-linearity into the network. This allows the network to approximate any continuous function, making it capable of learning intricate patterns and decision boundaries.\n",
        "2. Enabling Complex Representations:\n",
        "\n",
        "Feature Extraction: Hidden layers learn increasingly complex features from the input data. Activation functions allow these layers to extract non-linear features, which are crucial for representing the underlying structure of the data.\n",
        "Decision Boundaries: Non-linear activation functions enable the network to create non-linear decision boundaries, allowing it to classify data that is not linearly separable.\n",
        "3. Improving Gradient Flow:\n",
        "\n",
        "Vanishing Gradients: Certain activation functions, like sigmoid, can suffer from vanishing gradients, especially in deep networks. This can hinder the learning process.\n",
        "ReLU and its Variants: Activation functions like ReLU and its variants (Leaky ReLU, Parametric ReLU) help mitigate the vanishing gradient problem by introducing sparsity and ensuring a non-zero gradient for positive inputs.\n",
        "'''"
      ],
      "metadata": {
        "id": "oUID-hkb7gCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Explain the choice of activation functions for different types of problems (e.g., classification, regression) in the output layer\n",
        "'''1. Classification:\n",
        "\n",
        "Binary Classification:\n",
        "Sigmoid: A common choice for binary classification. It outputs a value between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class.1\n",
        "Advantages: Simple and straightforward interpretation.\n",
        "Limitations: Can suffer from vanishing gradients.\n",
        "Multi-class Classification:\n",
        "\n",
        "Softmax: Outputs a probability distribution over all possible classes. The output for each class is a value between 0 and 1, and the sum of all outputs equals 1.\n",
        "Advantages: Provides a well-calibrated probability distribution over the classes.\n",
        "Limitations: Can be computationally more expensive than other activation functions.\n",
        "\n",
        "2. Regression:\n",
        "\n",
        "Linear Activation:\n",
        "\n",
        "Identity Function (f(x) = x): Often used for regression problems where the output is not constrained to a specific range.\n",
        "Advantages: Simple and computationally efficient.\n",
        "Limitations: May not be suitable for all regression problems, especially those with bounded outputs.'''"
      ],
      "metadata": {
        "id": "NFmAUZRT70O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network architecture. Compare their effects on convergence and performance\n"
      ],
      "metadata": {
        "id": "LH4lL3lD80ER"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the first model (ReLU)\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Create an optimizer instance\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the first model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Train the first model (ReLU)\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the second model (Sigmoid)\n",
        "model_sigmoid = Sequential()\n",
        "model_sigmoid.add(Dense(units=64, activation='sigmoid', input_shape=(784,)))\n",
        "model_sigmoid.add(Dense(units=32, activation='sigmoid'))\n",
        "model_sigmoid.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Create a new optimizer instance for the second model\n",
        "optimizer_sigmoid = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the second model\n",
        "model_sigmoid.compile(loss='categorical_crossentropy', optimizer=optimizer_sigmoid, metrics=['accuracy'])\n",
        "\n",
        "# Train the second model (Sigmoid)\n",
        "model_sigmoid.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the third model (Tanh)\n",
        "model_tanh = Sequential()\n",
        "model_tanh.add(Dense(units=64, activation='tanh', input_shape=(784,)))\n",
        "model_tanh.add(Dense(units=32, activation='tanh'))\n",
        "model_tanh.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Create a new optimizer instance for the third model\n",
        "optimizer_tanh = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the third model\n",
        "model_tanh.compile(loss='categorical_crossentropy', optimizer=optimizer_tanh, metrics=['accuracy'])\n",
        "\n",
        "# Train the third model (Tanh)\n",
        "model_tanh.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the models\n",
        "test_loss_relu, test_acc_relu = model.evaluate(x_test, y_test)\n",
        "test_loss_sigmoid, test_acc_sigmoid = model_sigmoid.evaluate(x_test, y_test)\n",
        "test_loss_tanh, test_acc_tanh = model_tanh.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"ReLU: Test Accuracy:\", test_acc_relu)\n",
        "print(\"Sigmoid: Test Accuracy:\", test_acc_sigmoid)\n",
        "print(\"Tanh: Test Accuracy:\", test_acc_tanh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukPzd-7K9mtT",
        "outputId": "b82f3bed-12ea-4ef4-c07c-0ce98d0e6e4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8498 - loss: 0.5204 - val_accuracy: 0.9502 - val_loss: 0.1777\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1487 - val_accuracy: 0.9562 - val_loss: 0.1439\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.1044 - val_accuracy: 0.9665 - val_loss: 0.1110\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.0770 - val_accuracy: 0.9682 - val_loss: 0.1105\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9786 - loss: 0.0680 - val_accuracy: 0.9723 - val_loss: 0.0944\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0525 - val_accuracy: 0.9716 - val_loss: 0.0977\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0465 - val_accuracy: 0.9716 - val_loss: 0.0971\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9859 - loss: 0.0420 - val_accuracy: 0.9716 - val_loss: 0.0983\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0335 - val_accuracy: 0.9755 - val_loss: 0.0981\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.0313 - val_accuracy: 0.9737 - val_loss: 0.1002\n",
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7592 - loss: 1.0980 - val_accuracy: 0.9271 - val_loss: 0.2720\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9318 - loss: 0.2416 - val_accuracy: 0.9484 - val_loss: 0.1853\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9495 - loss: 0.1742 - val_accuracy: 0.9551 - val_loss: 0.1523\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1323 - val_accuracy: 0.9623 - val_loss: 0.1309\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1077 - val_accuracy: 0.9646 - val_loss: 0.1143\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.0901 - val_accuracy: 0.9675 - val_loss: 0.1096\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0753 - val_accuracy: 0.9699 - val_loss: 0.1012\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9804 - loss: 0.0678 - val_accuracy: 0.9700 - val_loss: 0.0954\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0577 - val_accuracy: 0.9715 - val_loss: 0.0913\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9863 - loss: 0.0489 - val_accuracy: 0.9726 - val_loss: 0.0927\n",
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8612 - loss: 0.5087 - val_accuracy: 0.9474 - val_loss: 0.1779\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1519 - val_accuracy: 0.9658 - val_loss: 0.1192\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.1000 - val_accuracy: 0.9689 - val_loss: 0.1048\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0744 - val_accuracy: 0.9693 - val_loss: 0.1031\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9826 - loss: 0.0577 - val_accuracy: 0.9724 - val_loss: 0.0877\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9863 - loss: 0.0467 - val_accuracy: 0.9722 - val_loss: 0.0894\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9887 - loss: 0.0378 - val_accuracy: 0.9733 - val_loss: 0.0870\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0327 - val_accuracy: 0.9732 - val_loss: 0.0890\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9913 - loss: 0.0277 - val_accuracy: 0.9739 - val_loss: 0.0934\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0240 - val_accuracy: 0.9736 - val_loss: 0.0978\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9682 - loss: 0.1233\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9692 - loss: 0.1027\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9701 - loss: 0.1076\n",
            "ReLU: Test Accuracy: 0.9736999869346619\n",
            "Sigmoid: Test Accuracy: 0.972599983215332\n",
            "Tanh: Test Accuracy: 0.9735999703407288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkntTuKXAH6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}